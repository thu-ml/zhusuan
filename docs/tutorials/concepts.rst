Basic Concepts in ZhuSuan
=========================

.. _dist-and-stochastic:

Distribution and StochasticTensor
---------------------------------

Probabilistic distributions are key components for building directed graphical
models (Bayesian Networks). ZhuSuan provides two layers of abstraction
for them: :class:`~zhusuan.distributions.base.Distribution` and
:class:`~zhusuan.model.base.StochasticTensor`, which may be
a little confusing for beginners. We make their definitions and connections
clear here.

Distribution
^^^^^^^^^^^^

The :class:`~zhusuan.distributions.base.Distribution` class is the base class
for various probabilistic distributions which support batch inputs, generating
batches of samples and evaluate probabilities at batches of given values.

The list of all available distributions can be found on these pages:

* :mod:`univariate distributions <zhusuan.distributions.univariate>`
* :mod:`multivariate distributions <zhusuan.distributions.multivariate>`

These distributions can be accessed from ZhuSuan by (for example, a univariate
Normal distribution)::

    >>> import zhusuan as zs
    >>> a = zs.distributions.Normal(mean=0., logstd=0.)

The typical input shape for a :class:`~zhusuan.distributions.base.Distribution`
is like ``batch_shape + input_shape``. where ``input_shape`` represents the
shape of a non-batch input parameter;
:attr:`~.Distribution.batch_shape` represents how many independent inputs are
fed into the distribution. In general, distributions support broadcasting for
inputs.

Samples can be generated by calling
:meth:`~zhusuan.distributions.base.Distribution.sample` method of distribution
objects. The shape is ``([n_samples] + )batch_shape + value_shape``.
The first additional axis is omitted only when passed `n_samples` is None
(by default), in which case one sample is generated. ``value_shape`` is the
non-batch value shape of the distribution. For a univariate distribution,
its ``value_shape`` is [].

Example of univariate distributions
(:class:`~zhusuan.distributions.univariate.Normal`)::

    >>> import tensorflow as tf
    >>> _ = tf.InteractiveSession()

    >>> b = zs.distributions.Normal([[-1., 1.], [0., -2.]], [0., 1.])

    >>> b.batch_shape.eval()
    array([2, 2], dtype=int32)

    >>> b.value_shape.eval()
    array([], dtype=int32)

    >>> tf.shape(b.sample()).eval()
    array([2, 2], dtype=int32)

    >>> tf.shape(b.sample(1)).eval()
    array([1, 2, 2], dtype=int32)

    >>> tf.shape(b.sample(10)).eval()
    array([10,  2,  2], dtype=int32)

Example of multivariate distributions
(:class:`~zhusuan.distributions.multivariate.OnehotCategorical`)::

    >>> c = zs.distributions.OnehotCategorical([[0., 1., -1.],
    ...                                         [2., 3., 4.]])

    >>> c.batch_shape.eval()
    array([2], dtype=int32)

    >>> c.value_shape.eval()
    array([3], dtype=int32)

    >>> tf.shape(c.sample()).eval()
    array([2, 3], dtype=int32)

    >>> tf.shape(c.sample(1)).eval()
    array([1, 2, 3], dtype=int32)

    >>> tf.shape(c.sample(10)).eval()
    array([10,  2,  3], dtype=int32)

There are cases where a batch of random variables are grouped into a
single event so that their probabilities can be computed together. This
is achieved by setting `group_ndims` argument, which defaults to 0.
The last `group_ndims` number of axes in
:attr:`~.Distribution.batch_shape` are grouped into a single event.
For example, ``Normal(..., group_ndims=1)`` will
set the last axis of its :attr:`~.Distribution.batch_shape` to a single event,
i.e., a multivariate Normal with identity covariance matrix.

Log probability density (mass) function can be evaluated by passing given
values to :meth:`~zhusuan.distributions.base.Distribution.log_prob` method of
distribution objects.
In that case, the given Tensor should be
broadcastable to shape ``(... + )batch_shape + value_shape``. The returned
Tensor has shape ``(... + )batch_shape[:-group_ndims]``. For example::

    >>> d = zs.distributions.Normal([[-1., 1.], [0., -2.]], 0.,
    ...                             group_ndims=1)

    >>> d.log_prob(0.).eval()
    array([-2.83787704, -3.83787727], dtype=float32)

    >>> e = zs.distributions.Normal(tf.zeros([2, 1, 3]), 0.,
    ...                             group_ndims=2)

    >>> tf.shape(e.log_prob(tf.zeros([5, 1, 1, 3]))).eval()
    array([5, 2], dtype=int32)

StochasticTensor
^^^^^^^^^^^^^^^^

While :class:`~zhusuan.distributions.base.Distribution` provides the basic
functionality for probabilistic distributions. Directly building computation
graph with them is still painful because they are not aware of any inner
reusability as stochastic nodes in Bayesian Networks: Once you have sampled
from a distribution, there is no way to reuse the downroot graph when you
want to observe it.

To address this challenge, ZhuSuan provides another abstraction built upon
distributions. That's :class:`~zhusuan.model.base.StochasticTensor`. For all
distributions available in :mod:`zhusuan.distributions` there is a
corresponding :class:`~zhusuan.model.base.StochasticTensor`, which can be
accessed by ``zs.Normal`` (for example, a univariate
:class:`~zhusuan.model.stochastic.Normal` StochasticTensor).
Their list is on :mod:`this page <zhusuan.model.stochastic>`.

:class:`~zhusuan.model.base.StochasticTensor` can only be constructed under
:class:`~zhusuan.model.base.BayesianNet` context.
Their instances are Tensor-like, which enables transparent building of Bayesian
Networks using tensorflow primitives. See the :ref:`bayesian-net` section for
examples of usage.

.. Note::

    Use ``zs.Normal`` when you want
    :class:`~zhusuan.model.base.StochasticTensor` and use
    ``zs.distributions.Normal`` when you want
    :class:`~zhusuan.distributions.base.Distribution`.

.. _bayesian-net:

BayesianNet
-----------

The :class:`~zhusuan.model.base.BayesianNet` class is a context class
supporting model construction in ZhuSuan as Bayesian Networks (Directed
graphical models). A :class:`~zhusuan.model.base.BayesianNet` represents a DAG
with two kinds of nodes:

* Deterministic nodes, made up of any tensorflow operations.
* Stochastic nodes, constructed by
  :class:`~zhusuan.model.base.StochasticTensor`.

To start a :class:`~zhusuan.model.base.BayesianNet` context::

    import zhusuan as zs
    with zs.BayesianNet() as model:
        # build the model

A Bayesian Linear Regression example:

.. math::

    w \sim N(0, \alpha^2 I)

    y \sim N(w^Tx, \beta^2)

::

    import tensorflow as tf
    import zhusuan as zs

    def bayesian_linear_regression(x, alpha, beta):
        with zs.BayesianNet() as model:
            w = zs.Normal('w', mean=0., logstd=tf.log(alpha)
            y_mean = tf.reduce_sum(tf.expand_dims(w, 0) * x, 1)
            y = zs.Normal('y', y_mean, tf.log(beta))
        return model

To observe any stochastic nodes in the network, pass a dictionary mapping of
``(name, Tensor)`` pairs when constructing
:class:`~zhusuan.model.base.BayesianNet`. This will assign observed values
to corresponding :class:`~zhusuan.model.base.StochasticTensor` s. For example::

    def bayesian_linear_regression(observed, x, alpha, beta):
        with zs.BayesianNet(observed=observed) as model:
            w = zs.Normal('w', mean=0., logstd=tf.log(alpha)
            y_mean = tf.reduce_sum(tf.expand_dims(w, 0) * x, 1)
            y = zs.Normal('y', y_mean, tf.log(beta))
        return model

    model = bayesian_linear_regression({'w': w_obs}, ...)

will set ``w`` to be observed. The result is that ``y_mean`` is computed
from the observed value of ``w`` (``w_obs``) instead of the samples of ``w``.
Calling the above function with different `observed` arguments instantiates the
:class:`~zhusuan.model.base.BayesianNet` with different observations, which
is a common behaviour for probabilistic graphical models.

.. Note::

    The observation passed must have the same type and shape as the
    `StochasticTensor`.

If there are
tensorflow `Variables <https://www.tensorflow.org/api_docs/python/tf/Variable>`_
created in this function, you may want to reuse it. ZhuSuan provides an easy
way to do this. You could just add a decorator to the function::

    @zs.reuse(scope="model")
    def bayesian_linear_regression(observed, x, alpha, beta):
        ...

See :func:`~zhusuan.model.base.reuse` for details.

After construction, :class:`~zhusuan.model.base.BayesianNet` supports queries
on the network::

    # get samples of random variable y following generative process
    # in the network
    model.outputs('y')

    # because w is observed in this case, its observed value will be
    # returned
    model.outputs('w')

    # get local log probability values of w and y, which returns
    # log p(w) and log p(y|w, x)
    model.local_log_prob(['w', 'y'])

    # query many quantities at the same time
    model.query('w', outputs=True, local_log_prob=True)

.. bibliography:: ../refs.bib
    :style: unsrtalpha
    :keyprefix: concepts-
